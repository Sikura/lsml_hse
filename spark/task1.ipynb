{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import spark_setup\n",
    "spark_setup.setup_pyspark_env()\n",
    "import spark_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambari - http://10.0.1.21:8080\n",
      "All Applications - http://10.0.1.23:8088/cluster\n",
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc = spark_utils.get_spark_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Fake big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_string(length):\n",
    "    # return \"A\" * length\n",
    "    # return ''.join(random.choice(string.letters) for _ in xrange(length))\n",
    "    return np.random.random((length / 8,)).tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.81 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = generate_string(1024*1024)\n",
    "print len(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60 ms, sys: 8 ms, total: 68 ms\n",
      "Wall time: 71.3 ms\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 17.8 ms\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "%time dump = cPickle.dumps(generate_string(1024*1024))\n",
    "%time _ = cPickle.loads(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N_KEYS = 100\n",
    "MB_PER_KEY = 1000\n",
    "N_JOBS = 100\n",
    "\n",
    "def mapper(x):\n",
    "    for i in xrange(N_KEYS * MB_PER_KEY / N_JOBS):\n",
    "        key = random.randint(1, N_KEYS)\n",
    "        yield key, generate_string(1024 * 1024)\n",
    "\n",
    "rdd = (\n",
    "    sc\n",
    "    .parallelize(range(N_JOBS), N_JOBS)\n",
    "    .flatMap(mapper)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from pyspark import StorageLevel\n",
    "# rdd.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pickle and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -rm -r -skipTrash hdfs:///user/ubuntu/bigDataPickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 0 ns, total: 32 ms\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "# 3min 27s - 97.7 GB - 471 MB/sec\n",
    "%time rdd.saveAsPickleFile(\"hdfs:///user/ubuntu/bigDataPickles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.7 G  hdfs:///user/ubuntu/bigDataPickles\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h hdfs:///user/ubuntu/bigDataPickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 136 ms, sys: 108 ms, total: 244 ms\n",
      "Wall time: 18min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(\n",
    "    sc.pickleFile(\"hdfs:///user/ubuntu/bigDataPickles\")\n",
    "    .groupByKey()\n",
    "    .map(lambda (x, y): (x, max(y)))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: long (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "ss = (SparkSession\n",
    "      .builder\n",
    "      .appName(\"spark sql example\")\n",
    "      .getOrCreate())\n",
    "\n",
    "df = ss.createDataFrame(\n",
    "    rdd\n",
    "    .map(lambda (x, y): Row(key=x, value=y))\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# faster with uncompressed this time\n",
    "ss.conf.set(\"spark.sql.parquet.compression.codec\", \"uncompressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/ubuntu/bigData.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r -skipTrash hdfs:///user/ubuntu/bigData.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 5 min\n",
    "# http://events.linuxfoundation.org/sites/events/files/slides/ApacheCon%20BigData%20Europe%202016%20-%20Parquet%20in%20Practice%20%26%20Detail_0.pdf\n",
    "df.write.save(\"hdfs:///user/ubuntu/bigData.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.8 G  hdfs:///user/ubuntu/bigData.parquet\r\n",
      "97.7 G  hdfs:///user/ubuntu/bigDataPickles\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h hdfs:///user/ubuntu/bigData.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 900 ms, sys: 308 ms, total: 1.21 s\n",
      "Wall time: 16min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(\n",
    "    ss.read.parquet(\"hdfs:///user/ubuntu/bigData.parquet\")\n",
    "    .groupby(\"key\")\n",
    "    .agg({\"value\": \"max\"})\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dumb local job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 348 ms, total: 1min 28s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "def slow_mapper(x):\n",
    "    s = 0\n",
    "    for i in xrange(300):\n",
    "        s = np.sum(x)\n",
    "    return s\n",
    "\n",
    "count = map(slow_mapper, np.random.random((100000,1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dumb Spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.42 s, sys: 1.33 s, total: 2.75 s\n",
      "Wall time: 22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sc.parallelize(np.random.random((100000,1000)))\\\n",
    "    .map(slow_mapper)\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Simple MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[7] at RDD at PythonRDD.scala:48\n",
      "[('text', 2), ('this', 1), ('too', 1), ('is', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd = (\n",
    "    sc\n",
    "   .parallelize([\"this is text\", \"text too\"])\n",
    "   .flatMap(lambda x: [(w, 1) for w in x.split()])\n",
    "   .reduceByKey(lambda a, b: a + b))\n",
    "print rdd\n",
    "print rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Broadcast + Accumulator example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[13] at RDD at PythonRDD.scala:48\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "errors: 1\n"
     ]
    }
   ],
   "source": [
    "bc = sc.broadcast({\"this\": 0, \"is\": 1, \"text\": 2})\n",
    "errors = sc.accumulator(0)\n",
    "\n",
    "def mapper(x):\n",
    "    global errors\n",
    "    for w in x.split():\n",
    "        if w in bc.value:\n",
    "            yield (bc.value[w], 1)\n",
    "        else:\n",
    "            errors += 1\n",
    "\n",
    "rdd = (\n",
    "    sc\n",
    "   .parallelize([\"this is text\", \"text too\"])\n",
    "   .flatMap(mapper)\n",
    "   .reduceByKey(lambda a, b: a + b))\n",
    "print rdd\n",
    "print rdd.collect()\n",
    "print \"errors:\", errors.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DataFrame API example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = (SparkSession\n",
    "      .builder\n",
    "      .appName(\"spark sql example\")\n",
    "      .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [[\"cat\", [1, 1]], [\"cat\", [2]], [\"dog\", [1]]], \n",
    "    columns=[\"name\", \"cnt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name     cnt\n",
       "0  cat  [1, 1]\n",
       "1  cat     [2]\n",
       "2  dog     [1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdf = ss.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- cnt: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdf.registerTempTable(\"animals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  sum\n",
       "0  dog    1\n",
       "1  cat    4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "select name, sum(cnt) as sum\n",
    "from \n",
    "    (select name, explode(cnt) as cnt\n",
    "     from animals)\n",
    "group by name\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Kill workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
