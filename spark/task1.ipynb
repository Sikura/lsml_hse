{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import spark_setup\n",
    "spark_setup.setup_pyspark_env()\n",
    "import spark_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambari - http://10.0.1.21:8080\n",
      "All Applications - http://10.0.1.23:8088/cluster\n",
      "CPU times: user 20 ms, sys: 8 ms, total: 28 ms\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc = spark_utils.get_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "hdfs_client = InsecureClient(\"http://cluster1:50070\", user='hdfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load data to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/outbrain-click-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        print '%r (%r, %r) %2.2f sec' % \\\n",
    "              (method.__name__, args, kw, te-ts)\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_client.delete(\"/task1\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'unzip_to_hdfs' (('clicks_test.csv.zip',), {}) 7.98 sec\n",
      "\n",
      "'unzip_to_hdfs' (('clicks_train.csv.zip',), {}) 21.13 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_categories.csv.zip',), {}) 3.34 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_entities.csv.zip',), {}) 6.14 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_meta.csv.zip',), {}) 2.91 sec\n",
      "\n",
      "'unzip_to_hdfs' (('documents_topics.csv.zip',), {}) 6.78 sec\n",
      "\n",
      "'unzip_to_hdfs' (('events.csv.zip',), {}) 23.32 sec\n",
      "\n",
      "'unzip_to_hdfs' (('page_views.csv.zip',), {}) 1358.40 sec\n",
      "\n",
      "'unzip_to_hdfs' (('page_views_sample.csv.zip',), {}) 8.06 sec\n",
      "\n",
      "'unzip_to_hdfs' (('promoted_content.csv.zip',), {}) 2.25 sec\n",
      "\n",
      "'unzip_to_hdfs' (('sample_submission.csv.zip',), {}) 7.14 sec\n",
      "CPU times: user 168 ms, sys: 84 ms, total: 252 ms\n",
      "Wall time: 24min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "\n",
    "@timeit\n",
    "def unzip_to_hdfs(fn):\n",
    "    fn_out = fn.replace(\".zip\", \"\")\n",
    "    print subprocess.check_output(\"unzip -p /data/{0} | hadoop fs -put - /task1/{1}\".format(fn, fn_out), shell=True)\n",
    "    \n",
    "fns = [\n",
    "    \"clicks_test.csv.zip\",\n",
    "    \"clicks_train.csv.zip\",\n",
    "    \"documents_categories.csv.zip\",\n",
    "    \"documents_entities.csv.zip\",\n",
    "    \"documents_meta.csv.zip\",\n",
    "    \"documents_topics.csv.zip\",\n",
    "    \"events.csv.zip\",\n",
    "    \"page_views.csv.zip\",\n",
    "    \"page_views_sample.csv.zip\",\n",
    "    \"promoted_content.csv.zip\",\n",
    "    \"sample_submission.csv.zip\"\n",
    "]\n",
    "\n",
    "for fn in fns:\n",
    "    unzip_to_hdfs(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483.5 M  /task1/clicks_test.csv\n",
      "1.4 G  /task1/clicks_train.csv\n",
      "112.5 M  /task1/documents_categories.csv\n",
      "309.1 M  /task1/documents_entities.csv\n",
      "85.2 M  /task1/documents_meta.csv\n",
      "323.7 M  /task1/documents_topics.csv\n",
      "1.1 G  /task1/events.csv\n",
      "88.4 G  /task1/page_views.csv\n",
      "433.3 M  /task1/page_views_sample.csv\n",
      "13.2 M  /task1/promoted_content.csv\n",
      "260.5 M  /task1/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h /task1/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# files are written on cluster1 node only, need to balance HDFS on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancer bandwidth is set to 1000000000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfsadmin -setBalancerBandwidth 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.48 s, sys: 2.22 s, total: 8.7 s\n",
      "Wall time: 6min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! hdfs balancer -threshold 5 > balancer.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pvdf = ss.read.csv(\"/task1/page_views.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uuid', 'string'),\n",
       " ('document_id', 'string'),\n",
       " ('timestamp', 'string'),\n",
       " ('platform', 'string'),\n",
       " ('geo_location', 'string'),\n",
       " ('traffic_source', 'string')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n",
      "|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 ms, sys: 20 ms, total: 96 ms\n",
      "Wall time: 9min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2034275448"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pvdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet is faster than CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://events.linuxfoundation.org/sites/events/files/slides/ApacheCon%20BigData%20Europe%202016%20-%20Parquet%20in%20Practice%20%26%20Detail_0.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pvdf.write.parquet(\"/task1/page_views.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.3 G  /task1/page_views.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h /task1/page_views.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pvdf2 = ss.read.parquet(\"/task1/page_views.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geo_location=u'US>NY', count=420207),\n",
       " Row(geo_location=u'US>MS>673', count=849299),\n",
       " Row(geo_location=u'ES>07', count=139257),\n",
       " Row(geo_location=u'CO>02', count=274301),\n",
       " Row(geo_location=u'DZ', count=141209)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import display\n",
    "boo = pvdf2.groupBy(\"geo_location\").count().collect()\n",
    "display(boo[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geo_location=u'US>MS>673', count=849299),\n",
       " Row(geo_location=u'CO>02', count=274301),\n",
       " Row(geo_location=u'DZ', count=141209),\n",
       " Row(geo_location=u'US>MT>756', count=676540),\n",
       " Row(geo_location=u'IL>01', count=21174)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 20 ms, total: 116 ms\n",
      "Wall time: 10min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "boo = pvdf.groupBy(\"geo_location\").count().collect()\n",
    "display(boo[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test.parquet done\n",
      "clicks_train.parquet done\n",
      "documents_categories.parquet done\n",
      "documents_entities.parquet done\n",
      "documents_meta.parquet done\n",
      "documents_topics.parquet done\n",
      "events.parquet done\n",
      "page_views.parquet done\n",
      "page_views_sample.parquet done\n",
      "promoted_content.parquet done\n",
      "sample_submission.parquet done\n",
      "CPU times: user 96 ms, sys: 0 ns, total: 96 ms\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def convert_all_to_parquet():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_client.list(task_dir)\n",
    "    for fn in all_files:\n",
    "        if fn.endswith(\".csv\"):\n",
    "            fn_after = fn.replace(\".csv\", \".parquet\")\n",
    "            path_before = task_dir + fn\n",
    "            path_after = task_dir + fn_after\n",
    "            if fn_after not in all_files:\n",
    "                # generate parquet\n",
    "                df = ss.read.csv(path_before, header=True)\n",
    "                df.write.parquet(path_after)\n",
    "            # remove csv, we have parquet now\n",
    "            hdfs_client.delete(path_before)\n",
    "            print fn_after, \"done\"\n",
    "\n",
    "convert_all_to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.2 M  /task1/clicks_test.parquet\n",
      "367.5 M  /task1/clicks_train.parquet\n",
      "36.5 M  /task1/documents_categories.parquet\n",
      "184.0 M  /task1/documents_entities.parquet\n",
      "21.2 M  /task1/documents_meta.parquet\n",
      "183.3 M  /task1/documents_topics.parquet\n",
      "669.3 M  /task1/events.parquet\n",
      "47.3 G  /task1/page_views.parquet\n",
      "236.9 M  /task1/page_views_sample.parquet\n",
      "5.0 M  /task1/promoted_content.parquet\n",
      "184.2 M  /task1/sample_submission.parquet\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -du -s -h /task1/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### /task1/clicks_test.parquet ###############\n",
      "+----------+------+\n",
      "|display_id| ad_id|\n",
      "+----------+------+\n",
      "|  17805143|288388|\n",
      "+----------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/clicks_train.parquet ###############\n",
      "+----------+-----+-------+\n",
      "|display_id|ad_id|clicked|\n",
      "+----------+-----+-------+\n",
      "|         1|42337|      0|\n",
      "+----------+-----+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_categories.parquet ###############\n",
      "+-----------+-----------+----------------+\n",
      "|document_id|category_id|confidence_level|\n",
      "+-----------+-----------+----------------+\n",
      "|    1544588|       1513|     0.263546236|\n",
      "+-----------+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_entities.parquet ###############\n",
      "+-----------+--------------------+-----------------+\n",
      "|document_id|           entity_id| confidence_level|\n",
      "+-----------+--------------------+-----------------+\n",
      "|    1539011|e01ed0c4a3e8f8f35...|0.327269624728567|\n",
      "+-----------+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_meta.parquet ###############\n",
      "+-----------+---------+------------+-------------------+\n",
      "|document_id|source_id|publisher_id|       publish_time|\n",
      "+-----------+---------+------------+-------------------+\n",
      "|     325048|      822|         253|2013-02-27 00:00:00|\n",
      "+-----------+---------+------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_topics.parquet ###############\n",
      "+-----------+--------+------------------+\n",
      "|document_id|topic_id|  confidence_level|\n",
      "+-----------+--------+------------------+\n",
      "|     801028|     280|0.0148711250868194|\n",
      "+-----------+--------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/events.parquet ###############\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/page_views.parquet ###############\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|68fb8eb72c49c4|    1201414| 63621328|       3|       GB>F8|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/page_views_sample.parquet ###############\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|7504d9623fdc7e|        234| 72194818|       1|   US>CA>825|             1|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/promoted_content.parquet ###############\n",
      "+-----+-----------+-----------+-------------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|\n",
      "+-----+-----------+-----------+-------------+\n",
      "|    1|       6614|          1|            7|\n",
      "+-----+-----------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/sample_submission.parquet ###############\n",
      "+----------+--------------------+\n",
      "|display_id|               ad_id|\n",
      "+----------+--------------------+\n",
      "|  21960532|50582 190398 2293...|\n",
      "+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 28 ms, sys: 4 ms, total: 32 ms\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preview_all_files():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_client.list(task_dir)\n",
    "    for fn in all_files:\n",
    "        df = ss.read.parquet(task_dir + fn)\n",
    "        print \"#\" * 15 + \" {0} \".format(task_dir + fn) + \"#\" * 15\n",
    "        df.show(1)\n",
    "        \n",
    "preview_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
